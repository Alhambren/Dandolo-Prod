# Migration Guides to Dandolo.ai

> **Switch to the superior AI inference platform in minutes, not hours**

## Why Migrate to Dandolo?

Before diving into the technical migration steps, here's why thousands of developers have already made the switch:

### From Venice.ai Direct
- ‚úÖ **No more provider management** - We handle all the complexity
- ‚úÖ **Intelligent model routing** - Best model automatically selected
- ‚úÖ **Built-in error handling** - Automatic retries and failover
- ‚úÖ **Better developer experience** - Native SDKs and comprehensive docs

### From OpenRoute.ai  
- ‚úÖ **Transparent pricing** - No hidden markups or fees
- ‚úÖ **Better privacy** - Zero data retention by design
- ‚úÖ **Superior performance** - Direct Venice.ai integration
- ‚úÖ **Agent-first features** - Built for AI agents, not retrofitted

### From OpenAI API
- ‚úÖ **100% API compatibility** - Drop-in replacement
- ‚úÖ **50+ models available** - Not limited to OpenAI models
- ‚úÖ **Censorship resistance** - Decentralized infrastructure
- ‚úÖ **Better pricing** - Competitive rates across all models

---

## üöÄ Venice.ai to Dandolo Migration

### Migration Time: ~5 minutes

### Before: Venice.ai Direct Integration

```python
import requests
import time
import random

class VeniceClient:
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://api.venice.ai"
        
    def get_available_providers(self):
        # Manual provider discovery
        response = requests.get(
            f"{self.base_url}/providers",
            headers={"Authorization": f"Bearer {self.api_key}"}
        )
        return response.json()
    
    def select_best_provider(self, providers, model_type="chat"):
        # Manual provider selection logic
        available = [p for p in providers if p["status"] == "online"]
        if not available:
            raise Exception("No providers available")
        return random.choice(available)  # Basic selection
    
    def chat_completion(self, messages, model=None):
        providers = self.get_available_providers()
        provider = self.select_best_provider(providers)
        
        # Manual retry logic
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    f"{provider['endpoint']}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "messages": messages,
                        "model": model or "default"
                    },
                    timeout=30
                )
                
                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 429:
                    time.sleep(2 ** attempt)  # Exponential backoff
                    continue
                else:
                    response.raise_for_status()
                    
            except requests.exceptions.RequestException as e:
                if attempt == max_retries - 1:
                    raise
                time.sleep(1)
                continue
        
        raise Exception("All retries exhausted")

# Usage
client = VeniceClient("your-venice-api-key")
try:
    response = client.chat_completion([
        {"role": "user", "content": "Hello world"}
    ])
    print(response["choices"][0]["message"]["content"])
except Exception as e:
    print(f"Error: {e}")
```

### After: Dandolo Integration

```python
from dandolo import Dandolo

# That's it! Everything else is handled automatically
client = Dandolo(api_key="ak_your_dandolo_key")

response = client.chat.completions.create(
    messages=[{"role": "user", "content": "Hello world"}]
)
print(response.choices[0].message.content)
```

### Migration Checklist

- [ ] **Get Dandolo API Key**: Visit [dandolo.ai/dashboard](https://dandolo.ai/dashboard)
- [ ] **Install SDK**: `pip install dandolo-sdk`
- [ ] **Replace client initialization**: 3 lines ‚Üí 1 line
- [ ] **Remove provider management code**: Delete 50+ lines of complexity
- [ ] **Remove retry logic**: Built-in automatic retries
- [ ] **Test**: Same functionality, better reliability
- [ ] **Deploy**: Zero downtime migration

### Performance Comparison

| Metric | Venice.ai Direct | Dandolo |
|--------|------------------|---------|
| **Setup Code** | 50+ lines | 3 lines |
| **Error Handling** | Manual | Automatic |
| **Provider Selection** | Random/manual | Intelligent |
| **Retry Logic** | Custom implementation | Built-in |
| **Health Monitoring** | Not included | Real-time |
| **Failover** | Manual | Automatic |

---

## üéØ OpenRoute.ai to Dandolo Migration

### Migration Time: ~3 minutes

### Before: OpenRoute.ai Integration

```python
import openai
import json

# OpenRoute.ai setup
openai.api_base = "https://openrouter.ai/api/v1"
openai.api_key = "sk-or-your-openrouter-key"

# Hidden costs and limited transparency
def chat_with_pricing_uncertainty(messages, model="anthropic/claude-3-sonnet"):
    try:
        response = openai.ChatCompletion.create(
            model=model,  # Manual model management
            messages=messages,
            # No visibility into actual costs until billing
        )
        return response
    except Exception as e:
        # Basic error handling, no automatic failover
        print(f"Request failed: {e}")
        return None

# Usage
response = chat_with_pricing_uncertainty([
    {"role": "user", "content": "Analyze this data"}
])

if response:
    print(response.choices[0].message.content)
    # No real-time cost information
    print("Cost: Unknown until monthly bill")
```

### After: Dandolo Integration

```python
from dandolo import Dandolo

client = Dandolo(api_key="ak_your_dandolo_key")

# Transparent pricing and intelligent routing
response = client.chat.completions.create(
    messages=[{"role": "user", "content": "Analyze this data"}],
    model="auto-select"  # Intelligent model selection
)

print(response.choices[0].message.content)
print(f"Cost: ${response.metadata.cost_usd:.4f}")  # Real-time cost info
print(f"Model used: {response.metadata.actual_model}")  # Transparency
```

### Key Improvements

| Feature | OpenRoute.ai | Dandolo |
|---------|--------------|---------|
| **Pricing Transparency** | ‚ùå Hidden until bill | ‚úÖ Real-time cost display |
| **Model Intelligence** | ‚ùå Manual selection | ‚úÖ Auto-routing |
| **Provider Diversity** | ‚úÖ Multiple providers | ‚úÖ Venice.ai network |
| **Privacy** | ‚ùå Request logging | ‚úÖ Zero data retention |
| **Error Recovery** | ‚ùå Basic | ‚úÖ Intelligent failover |
| **Developer Tools** | ‚ùå REST only | ‚úÖ Native SDKs |

### Migration Steps

1. **Replace API endpoint**:
   ```python
   # Before
   openai.api_base = "https://openrouter.ai/api/v1"
   
   # After  
   client = Dandolo(api_key="ak_your_key")
   ```

2. **Update authentication**:
   ```python
   # Before
   openai.api_key = "sk-or-your-key"
   
   # After
   # Already handled in client initialization
   ```

3. **Modernize API calls**:
   ```python
   # Before
   response = openai.ChatCompletion.create(
       model="anthropic/claude-3-sonnet",
       messages=messages
   )
   
   # After
   response = client.chat.completions.create(
       messages=messages,
       model="auto-select"  # Or specify exact model
   )
   ```

---

## üîÑ OpenAI to Dandolo Migration

### Migration Time: ~2 minutes (100% API Compatible)

### Before: OpenAI API

```python
import openai

openai.api_key = "sk-your-openai-key"

# Limited to OpenAI models only
response = openai.ChatCompletion.create(
    model="gpt-4",  # Only OpenAI models available
    messages=[
        {"role": "user", "content": "Write code to analyze data"}
    ],
    max_tokens=1000,
    temperature=0.7
)

print(response.choices[0].message.content)
print(f"Model: {response.model}")  # Always OpenAI
print("Cost: Unknown until monthly bill")
```

### After: Dandolo (Method 1 - Drop-in Replacement)

```python
import openai

# Just change the endpoint - everything else identical!
openai.api_base = "https://dandolo.ai/v1"
openai.api_key = "ak_your_dandolo_key"

# Now access 50+ models instead of just OpenAI
response = openai.ChatCompletion.create(
    model="auto-select",  # Intelligent routing to best model
    messages=[
        {"role": "user", "content": "Write code to analyze data"}
    ],
    max_tokens=1000,
    temperature=0.7
)

print(response.choices[0].message.content)
print(f"Model: {response.model}")  # Could be Claude, Llama, etc.
print(f"Cost: ${response.metadata.cost_usd:.4f}")  # Real-time pricing
```

### After: Dandolo (Method 2 - Native SDK)

```python
from dandolo import Dandolo

client = Dandolo(api_key="ak_your_dandolo_key")

# Enhanced features beyond OpenAI compatibility
response = client.chat.completions.create(
    messages=[
        {"role": "user", "content": "Write code to analyze data"}
    ],
    model="auto-select",  # Intelligent model selection
    agent_enhanced=True,  # Advanced agent features
    cost_optimization=True,  # Optimize for cost
    quality_preference="balanced"  # Speed vs quality preference
)

print(response.choices[0].message.content)
print(f"Model: {response.metadata.actual_model}")
print(f"Cost: ${response.metadata.cost_usd:.4f}")
print(f"Processing time: {response.metadata.processing_time_ms}ms")
```

### Migration Benefits

| Aspect | OpenAI API | Dandolo |
|--------|------------|---------|
| **Model Access** | OpenAI only | 50+ models |
| **Pricing** | Premium | Competitive |
| **Availability** | Single provider | Multi-provider |
| **Privacy** | Data retained | Zero retention |
| **Censorship** | Centralized limits | Decentralized freedom |
| **API Compatibility** | Native | 100% compatible |

### Migration Paths

#### Path 1: Minimal Change (Recommended for quick migration)

```python
# Change only 2 lines in your existing code:
import openai

openai.api_base = "https://dandolo.ai/v1"  # Add this line
openai.api_key = "ak_your_dandolo_key"     # Change this line

# Everything else stays exactly the same!
```

#### Path 2: Native SDK (Recommended for new features)

```python
# Replace openai import with dandolo
from dandolo import Dandolo

client = Dandolo(api_key="ak_your_dandolo_key")

# Enhanced API with additional features
response = client.chat.completions.create(...)
```

---

## üîß Framework-Specific Migrations

### LangChain Migration

#### Before: OpenAI LangChain

```python
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

# Limited to OpenAI models
llm = ChatOpenAI(
    openai_api_key="sk-your-openai-key",
    model_name="gpt-4"
)
```

#### After: Dandolo LangChain

```python
from dandolo.integrations.langchain import DandoloLLM

# Access to 50+ models with intelligent routing
llm = DandoloLLM(
    api_key="ak_your_dandolo_key",
    model="auto-select"  # Or any specific model
)
```

### AutoGen Migration

#### Before: OpenAI AutoGen

```python
import autogen

config_list = [
    {
        "model": "gpt-4",
        "api_key": "sk-your-openai-key"
    }
]

assistant = autogen.AssistantAgent(
    name="assistant",
    llm_config={"config_list": config_list}
)
```

#### After: Dandolo AutoGen

```python
import autogen
from dandolo.integrations.autogen import DandoloConfig

config_list = DandoloConfig(
    api_key="ak_your_dandolo_key",
    model="auto-select"
).to_autogen_format()

assistant = autogen.AssistantAgent(
    name="assistant", 
    llm_config={"config_list": config_list}
)
```

### CrewAI Migration

#### Before: OpenAI CrewAI

```python
from crewai import Agent, LLM

llm = LLM(
    model="openai/gpt-4",
    api_key="sk-your-openai-key"
)

agent = Agent(
    role="Data Analyst",
    llm=llm
)
```

#### After: Dandolo CrewAI

```python
from crewai import Agent
from dandolo.integrations.crewai import DandoloLLM

llm = DandoloLLM(
    api_key="ak_your_dandolo_key",
    model="auto-select"  # Best model for the task
)

agent = Agent(
    role="Data Analyst",
    llm=llm
)
```

---

## üß™ Testing Your Migration

### Test Script Template

```python
import time
from dandolo import Dandolo

def test_migration():
    client = Dandolo(api_key="ak_your_test_key")
    
    # Test 1: Basic functionality
    try:
        response = client.chat.completions.create(
            messages=[{"role": "user", "content": "Hello, testing migration"}]
        )
        print("‚úÖ Basic chat works")
        print(f"   Response: {response.choices[0].message.content[:50]}...")
    except Exception as e:
        print(f"‚ùå Basic chat failed: {e}")
        return False
    
    # Test 2: Streaming
    try:
        stream = client.chat.completions.create(
            messages=[{"role": "user", "content": "Count to 3"}],
            stream=True
        )
        content = ""
        for chunk in stream:
            if chunk.choices[0].delta.content:
                content += chunk.choices[0].delta.content
        print("‚úÖ Streaming works")
        print(f"   Streamed: {content[:50]}...")
    except Exception as e:
        print(f"‚ùå Streaming failed: {e}")
    
    # Test 3: Model selection
    try:
        response = client.chat.completions.create(
            messages=[{"role": "user", "content": "What model are you?"}],
            model="auto-select"
        )
        print("‚úÖ Auto-select works")
        print(f"   Model used: {response.metadata.actual_model}")
    except Exception as e:
        print(f"‚ùå Auto-select failed: {e}")
    
    # Test 4: Usage tracking
    try:
        usage = client.usage()
        print("‚úÖ Usage tracking works")
        print(f"   Requests today: {usage.daily_usage.requests}/{usage.daily_usage.limit}")
    except Exception as e:
        print(f"‚ùå Usage tracking failed: {e}")
    
    print("\nüéâ Migration test complete!")
    return True

if __name__ == "__main__":
    test_migration()
```

### Performance Comparison Script

```python
import time
import statistics
from dandolo import Dandolo

def benchmark_performance():
    client = Dandolo(api_key="ak_your_key")
    
    test_message = [{"role": "user", "content": "Write a haiku about technology"}]
    response_times = []
    
    print("Running performance benchmark...")
    
    for i in range(10):
        start_time = time.time()
        
        response = client.chat.completions.create(
            messages=test_message,
            model="auto-select"
        )
        
        end_time = time.time()
        response_time = end_time - start_time
        response_times.append(response_time)
        
        print(f"Request {i+1}: {response_time:.2f}s")
        time.sleep(1)  # Rate limiting courtesy
    
    avg_time = statistics.mean(response_times)
    median_time = statistics.median(response_times)
    min_time = min(response_times)
    max_time = max(response_times)
    
    print(f"\nüìä Performance Results:")
    print(f"   Average: {avg_time:.2f}s")
    print(f"   Median: {median_time:.2f}s") 
    print(f"   Min: {min_time:.2f}s")
    print(f"   Max: {max_time:.2f}s")

if __name__ == "__main__":
    benchmark_performance()
```

---

## üö® Migration Troubleshooting

### Common Issues and Solutions

#### Issue: "Invalid API Key"
```python
# ‚ùå Wrong key format
api_key = "sk-openai-format-key"

# ‚úÖ Correct Dandolo key format
api_key = "ak_dandolo_agent_key"
```

#### Issue: "Model not found"
```python
# ‚ùå Using OpenAI-specific model names
model = "gpt-4"

# ‚úÖ Use auto-select or Venice.ai model names
model = "auto-select"  # Recommended
# or
model = "claude-3-sonnet-20240229"  # Specific model
```

#### Issue: "Rate limit exceeded"
```python
# ‚úÖ Check your current usage
usage = client.usage()
print(f"Used: {usage.daily_usage.requests}/{usage.daily_usage.limit}")

# ‚úÖ Implement proper retry logic
from dandolo.exceptions import RateLimitError
try:
    response = client.chat.completions.create(messages=messages)
except RateLimitError as e:
    print(f"Rate limited. Retry after: {e.retry_after} seconds")
```

### Support Channels

- **Migration Support**: [migration@dandolo.ai](mailto:migration@dandolo.ai)
- **Live Chat**: Available on [dandolo.ai](https://dandolo.ai)
- **Discord**: [discord.gg/dandolo](https://discord.gg/dandolo)
- **Documentation**: [docs.dandolo.ai/migration](https://docs.dandolo.ai/migration)

---

## üéØ Migration Success Stories

### "Reduced our AI infrastructure code by 80%"
*"We migrated from Venice.ai direct integration to Dandolo in 30 minutes. Our codebase went from 200+ lines of provider management to just 10 lines of clean, reliable code."*
‚Äî Sarah Chen, CTO at DataFlow AI

### "5x better reliability, 3x faster setup"  
*"OpenRoute.ai was costing us too much with hidden fees. Dandolo's transparent pricing and superior performance made the migration a no-brainer."*
‚Äî Marcus Rodriguez, Lead Engineer at AgentCore

### "Zero downtime migration from OpenAI"
*"The 100% API compatibility meant we changed 2 lines of code and instantly gained access to 50+ models. Our agents now have more capabilities and better uptime."*
‚Äî Dr. Emily Watson, AI Research Director

---

## üìã Migration Checklist

### Pre-Migration
- [ ] Review current API usage patterns
- [ ] Estimate cost savings with Dandolo pricing calculator
- [ ] Get Dandolo API key from [dandolo.ai/dashboard](https://dandolo.ai/dashboard)
- [ ] Test Dandolo in development environment

### During Migration
- [ ] Update API endpoints/imports
- [ ] Replace API keys
- [ ] Test core functionality
- [ ] Update error handling (if using native SDK)
- [ ] Verify streaming works (if used)

### Post-Migration
- [ ] Monitor performance and reliability
- [ ] Compare costs and usage
- [ ] Update documentation
- [ ] Train team on new features
- [ ] Celebrate improved developer experience! üéâ

---

*Need help with your migration? Contact our migration specialists at [migration@dandolo.ai](mailto:migration@dandolo.ai)*